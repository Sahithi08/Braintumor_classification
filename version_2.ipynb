{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 8036589,
          "sourceType": "datasetVersion",
          "datasetId": 4737688
        },
        {
          "sourceId": 8216048,
          "sourceType": "datasetVersion",
          "datasetId": 4869819
        },
        {
          "sourceId": 8216086,
          "sourceType": "datasetVersion",
          "datasetId": 4869846
        },
        {
          "sourceId": 8253580,
          "sourceType": "datasetVersion",
          "datasetId": 4897673
        },
        {
          "sourceId": 8253592,
          "sourceType": "datasetVersion",
          "datasetId": 4897683
        }
      ],
      "dockerImageVersionId": 30699,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-04-28T16:12:36.074313Z",
          "iopub.execute_input": "2024-04-28T16:12:36.074697Z",
          "iopub.status.idle": "2024-04-28T16:12:36.490559Z",
          "shell.execute_reply.started": "2024-04-28T16:12:36.074665Z",
          "shell.execute_reply": "2024-04-28T16:12:36.489644Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bh-Zpu0MzyaI",
        "outputId": "e378ee23-1d10-4256-a747-9a3558f337be"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openslide-python\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-28T16:12:39.394928Z",
          "iopub.execute_input": "2024-04-28T16:12:39.395438Z",
          "iopub.status.idle": "2024-04-28T16:12:52.209561Z",
          "shell.execute_reply.started": "2024-04-28T16:12:39.395405Z",
          "shell.execute_reply": "2024-04-28T16:12:52.208419Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2pSeQh2zyaL",
        "outputId": "a54b4df2-c36a-498a-de4e-13bcf3581012"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openslide-python\n",
            "  Downloading openslide-python-1.3.1.tar.gz (358 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/359.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/359.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m359.0/359.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from openslide-python) (9.4.0)\n",
            "Building wheels for collected packages: openslide-python\n",
            "  Building wheel for openslide-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openslide-python: filename=openslide_python-1.3.1-cp310-cp310-linux_x86_64.whl size=33544 sha256=d1ab28cb27812e577409ce9338a271786ff6a45149a6bfcafbe1ece2ac3320fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/79/fa/29a0087493c69dff7fd0b70fab5d6771002a531010161d2d97\n",
            "Successfully built openslide-python\n",
            "Installing collected packages: openslide-python\n",
            "Successfully installed openslide-python-1.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y openslide-tools\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-28T16:12:52.211505Z",
          "iopub.execute_input": "2024-04-28T16:12:52.211821Z",
          "iopub.status.idle": "2024-04-28T16:12:55.466802Z",
          "shell.execute_reply.started": "2024-04-28T16:12:52.211792Z",
          "shell.execute_reply": "2024-04-28T16:12:55.465749Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-J8v-uhzyaL",
        "outputId": "dff554da-794f-4732-ef09-77eada9e0c46"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libopenslide0\n",
            "Suggested packages:\n",
            "  libtiff-tools\n",
            "The following NEW packages will be installed:\n",
            "  libopenslide0 openslide-tools\n",
            "0 upgraded, 2 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 104 kB of archives.\n",
            "After this operation, 297 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopenslide0 amd64 3.4.1+dfsg-5build1 [89.8 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 openslide-tools amd64 3.4.1+dfsg-5build1 [13.8 kB]\n",
            "Fetched 104 kB in 1s (119 kB/s)\n",
            "Selecting previously unselected package libopenslide0.\n",
            "(Reading database ... 121920 files and directories currently installed.)\n",
            "Preparing to unpack .../libopenslide0_3.4.1+dfsg-5build1_amd64.deb ...\n",
            "Unpacking libopenslide0 (3.4.1+dfsg-5build1) ...\n",
            "Selecting previously unselected package openslide-tools.\n",
            "Preparing to unpack .../openslide-tools_3.4.1+dfsg-5build1_amd64.deb ...\n",
            "Unpacking openslide-tools (3.4.1+dfsg-5build1) ...\n",
            "Setting up libopenslide0 (3.4.1+dfsg-5build1) ...\n",
            "Setting up openslide-tools (3.4.1+dfsg-5build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openslide\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def load_images_from_folder(folder, label, resize_width=256, resize_height=256):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for filename in os.listdir(folder):\n",
        "        if filename.endswith(\".svs\"):\n",
        "            path = os.path.join(folder, filename)\n",
        "            slide = openslide.OpenSlide(path)\n",
        "            level_count = slide.level_count\n",
        "            level_dimensions = slide.level_dimensions\n",
        "            level = 0  # Assuming level 0 has the lowest resolution\n",
        "            width, height = level_dimensions[level]\n",
        "\n",
        "            # Starting dimensions for cropping\n",
        "            start_width = 0\n",
        "            start_height = 0\n",
        "            cropped_width = width // 5\n",
        "            cropped_height = height // 5\n",
        "\n",
        "            # Loop through 5 rows and columns to create 25 cropped images\n",
        "            for row in range(5):\n",
        "                for col in range(5):\n",
        "                    # Load a region of the slide\n",
        "                    region = slide.read_region(location=(start_width, start_height), level=level, size=(cropped_width, cropped_height))\n",
        "                    # Resize the image\n",
        "                    region = region.resize((resize_width, resize_height))\n",
        "                    image = region.convert(\"RGB\")\n",
        "\n",
        "                    images.append(image)\n",
        "                    labels.append(label)\n",
        "\n",
        "                    # Update starting dimensions for next cropped image\n",
        "                    start_width += cropped_width\n",
        "                start_height += cropped_height\n",
        "                start_width = 0  # Reset start width for next row\n",
        "\n",
        "            # Close the slide\n",
        "            slide.close()\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "# Load LGG and GBM images with labels\n",
        "train_lgg_images, train_lgg_labels = load_images_from_folder(\"/content/drive/MyDrive/pe_ramesh_sir/lgg\", label=\"lgg\")\n",
        "train_gbm_images, train_gbm_labels = load_images_from_folder(\"/content/drive/MyDrive/pe_ramesh_sir/gbm\", label=\"gbm\")\n",
        "test_images, _ = load_images_from_folder(\"/content/drive/MyDrive/pe_ramesh_sir/test\", label=None)\n",
        "\n",
        "# You can adjust the resize_width and resize_height parameters as needed\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-28T16:41:50.576114Z",
          "iopub.execute_input": "2024-04-28T16:41:50.576401Z",
          "iopub.status.idle": "2024-04-28T17:01:55.029568Z",
          "shell.execute_reply.started": "2024-04-28T16:41:50.576375Z",
          "shell.execute_reply": "2024-04-28T17:01:55.028736Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "EMrI0Rj-zyaM",
        "outputId": "7397d242-fd6a-4672-a6a0-b7fb910a3b85"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'openslide'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-8085077ef0db>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mopenslide\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_images_from_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize_height\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openslide'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "zoxD8tQvzyaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import numpy as np\n",
        "import multiprocessing\n",
        "\n",
        "# Load CLIP model and processor\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Move model to appropriate device\n",
        "clip_model.to(device)\n",
        "\n",
        "def process_image(image, label):\n",
        "    image_features = []\n",
        "    inputs = clip_processor(text=label, images=image, return_tensors=\"pt\")\n",
        "    inputs = {key: tensor.to(device) for key, tensor in inputs.items()}  # Move inputs to device\n",
        "    with torch.no_grad():\n",
        "        image_feature = clip_model.get_image_features(pixel_values=inputs[\"pixel_values\"])\n",
        "    image_features.append(image_feature.cpu().numpy())\n",
        "    return np.concatenate(image_features, axis=0)\n",
        "\n",
        "def extract_features(images, labels, batch_size=1):\n",
        "    features = []\n",
        "\n",
        "    pool = multiprocessing.Pool()\n",
        "    results = pool.starmap(process_image, zip(images, labels))\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "    return np.array(results)\n",
        "\n",
        "# Extract features for training and testing images\n",
        "train_lgg_features = extract_features(train_lgg_images, train_lgg_labels)\n",
        "train_gbm_features = extract_features(train_gbm_images, train_gbm_labels)\n",
        "test_features = extract_features(test_images, [None]*len(test_images))\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-28T17:05:32.331098Z",
          "iopub.execute_input": "2024-04-28T17:05:32.331514Z"
        },
        "trusted": true,
        "id": "Md4KNIe6zyaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Combine features and labels\n",
        "X_train_flat = np.concatenate((train_lgg_features, train_gbm_features), axis=0)\n",
        "# Flatten the second dimension\n",
        "X_train_flat = X_train_flat.reshape(X_train_flat.shape[0], -1)\n",
        "y_train = np.concatenate((train_lgg_labels, train_gbm_labels))\n",
        "\n",
        "\n",
        "print(\"Shape of X_train_flat:\", X_train_flat.shape)\n",
        "\n",
        "\n",
        "# Train logistic regression classifier\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train_flat, y_train)\n",
        "\n",
        "# Make predictions based on majority voting\n",
        "def predict_majority(features):\n",
        "    num_images = len(features)\n",
        "    num_cropped_images = features.shape[0] // num_images  # Calculate based on the shape of features\n",
        "    print(\"Number of cropped images per original image:\", num_cropped_images)\n",
        "\n",
        "    # Reshape features to (num_images * num_cropped_images, feature_dim)\n",
        "    features_reshaped = features.reshape(-1, features.shape[-1])\n",
        "\n",
        "    final_predictions = []\n",
        "\n",
        "    for i in range(0, len(features_reshaped), num_cropped_images):\n",
        "        # Predict labels for each cropped image of the current original image\n",
        "        predictions = classifier.predict(features_reshaped[i:i+num_cropped_images])\n",
        "\n",
        "        # Determine majority vote for the current original image\n",
        "        majority_votes = np.sum(predictions == \"gbm\") > (num_cropped_images // 2)\n",
        "\n",
        "        # Convert majority vote to label for the current original image\n",
        "        final_prediction = \"gbm\" if majority_votes else \"lgg\"\n",
        "\n",
        "        # Append final prediction for the current original image\n",
        "        final_predictions.append(final_prediction)\n",
        "\n",
        "    return np.array(final_predictions)\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate performance\n",
        "def evaluate(predictions, true_labels):\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    return accuracy\n",
        "\n",
        "# Evaluate on training set\n",
        "train_predictions = predict_majority(np.concatenate((train_lgg_features, train_gbm_features), axis=0))\n",
        "train_accuracy = evaluate(train_predictions, y_train)\n",
        "print(\"Training Accuracy:\", train_accuracy)"
      ],
      "metadata": {
        "trusted": true,
        "id": "fhXk038LzyaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on test set using majority voting\n",
        "test_predictions = predict_majority(test_features)\n",
        "\n",
        "# Display predictions\n",
        "for i, prediction in enumerate(test_predictions):\n",
        "    if prediction == \"lgg\":\n",
        "        print(f\"Test image {i+1} is classified as LGG.\")\n",
        "    else:\n",
        "        print(f\"Test image {i+1} is classified as GBM.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "buEGs9SQzyaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Obtain true labels for the test set\n",
        "# # Assuming you have the true labels stored in a variable called test_labels\n",
        "# # Replace 'test_labels' with the actual variable name if it's different\n",
        "# test_labels = ['lgg', 'lgg','gbm','gbm']  # Example of test labels, replace this with your actual test labels\n",
        "\n",
        "# # Make predictions on test set\n",
        "# test_predictions = predict(np.concatenate(test_features))\n",
        "\n",
        "# # Calculate testing accuracy\n",
        "# test_accuracy = evaluate(test_predictions, test_labels)\n",
        "# print(\"Testing Accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "b6Yx_2yszyaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain true labels for the test set\n",
        "# Assuming you have the true labels stored in a variable called test_labels\n",
        "# Replace 'test_labels' with the actual variable name if it's different\n",
        "test_labels = ['lgg', 'lgg', 'gbm', 'gbm']  # Example of test labels, replace this with your actual test labels\n",
        "\n",
        "# Make predictions on test set using majority voting\n",
        "test_predictions = predict_majority(test_features)\n",
        "\n",
        "# Calculate testing accuracy\n",
        "test_accuracy = evaluate(test_predictions, test_labels)\n",
        "print(\"Testing Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "trusted": true,
        "id": "I1Cnc6qHzyaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the feature vectors\n",
        "test_features_flat = np.vstack(test_features)\n",
        "\n",
        "# Make predictions on the test set\n",
        "test_predictions = classifier.predict(test_features_flat)\n",
        "\n",
        "# Calculate accuracy\n",
        "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision_test = precision_score(test_labels, test_predictions, average='weighted')\n",
        "recall_test = recall_score(test_labels, test_predictions, average='weighted')\n",
        "f1_test = f1_score(test_labels, test_predictions, average='weighted')\n",
        "print(\"Precision (Test):\", precision_test)\n",
        "print(\"Recall (Test):\", recall_test)\n",
        "print(\"F1-score (Test):\", f1_test)\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix_test = confusion_matrix(test_labels, test_predictions)\n",
        "print(\"Confusion Matrix (Test):\")\n",
        "print(conf_matrix_test)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "nZbYzzbgzyaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (true_label, pred_label) in enumerate(zip(test_labels, test_predictions)):\n",
        "    print(f\"Sample {i+1}: True label - {true_label}, Predicted label - {pred_label}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "P4x-0nfizyaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCA\n"
      ],
      "metadata": {
        "id": "rw9d9xi1zyaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the feature vectors\n",
        "all_features_flat = all_features.reshape(all_features.shape[0], -1)\n",
        "\n",
        "# Apply PCA for dimensionality reduction\n",
        "pca = PCA(n_components=2)\n",
        "pca_features = pca.fit_transform(all_features_flat)\n",
        "\n",
        "# Plot the PCA features\n",
        "plt.figure(figsize=(10, 6))\n",
        "for label in np.unique(all_labels):\n",
        "    plt.scatter(pca_features[all_labels == label, 0], pca_features[all_labels == label, 1], label=label)\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('PCA Visualization of CLIP Features')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "nBn_-2txzyaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  cross-validation"
      ],
      "metadata": {
        "id": "LKY5DOojzyaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Define the number of folds for cross-validation\n",
        "n_splits = 5  # You can adjust this as needed\n",
        "\n",
        "# Define cross-validation strategy\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize lists to store accuracy scores for each fold\n",
        "cv_train_accuracies = []\n",
        "cv_val_accuracies = []\n",
        "\n",
        "# Perform cross-validation\n",
        "for fold, (train_index, val_index) in enumerate(skf.split(X_train_flat, y_train), 1):\n",
        "    print(f\"Fold {fold}/{n_splits}:\")\n",
        "    # Split data into training and validation sets\n",
        "    X_train_fold, X_val_fold = X_train_flat[train_index], X_train_flat[val_index]\n",
        "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        "\n",
        "    # Train logistic regression classifier\n",
        "    classifier = LogisticRegression()\n",
        "    classifier.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "    # Make predictions on training and validation sets\n",
        "    train_predictions = classifier.predict(X_train_fold)\n",
        "    val_predictions = classifier.predict(X_val_fold)\n",
        "\n",
        "    # Calculate accuracy scores\n",
        "    train_accuracy = accuracy_score(y_train_fold, train_predictions)\n",
        "    val_accuracy = accuracy_score(y_val_fold, val_predictions)\n",
        "\n",
        "    print(f\"Training Accuracy: {train_accuracy}\")\n",
        "    print(f\"Validation Accuracy: {val_accuracy}\\n\")\n",
        "\n",
        "    # Store accuracy scores\n",
        "    cv_train_accuracies.append(train_accuracy)\n",
        "    cv_val_accuracies.append(val_accuracy)\n",
        "\n",
        "# Calculate average accuracy across all folds\n",
        "avg_train_accuracy = np.mean(cv_train_accuracies)\n",
        "avg_val_accuracy = np.mean(cv_val_accuracies)\n",
        "\n",
        "print(f\"Average Training Accuracy across all folds: {avg_train_accuracy}\")\n",
        "print(f\"Average Validation Accuracy across all folds: {avg_val_accuracy}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "9CJVnkNTzyaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "u5g0tl4izyaP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5-V_pmvWzyaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LCC4F5V3zyaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wHn1kGp1zyaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ek-CLBkNzyaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CQg-deeCzyaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Euz3EPA2zyaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dqhPlpFJzyaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vcpYYWBZzyaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Augmentation: Apply data augmentation techniques such as rotation, flipping, scaling, or adding noise to increase the diversity of your training data.\n"
      ],
      "metadata": {
        "id": "Rm1E_N86zyaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openslide\n",
        "import os\n",
        "import openslide\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define data augmentation transforms\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "def load_images_from_folder(folder, label):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for filename in os.listdir(folder):\n",
        "        if filename.endswith(\".svs\"):\n",
        "            path = os.path.join(folder, filename)\n",
        "            slide = openslide.OpenSlide(path)\n",
        "            level_count = slide.level_count\n",
        "            level_dimensions = slide.level_dimensions\n",
        "            level = 0  # Assuming level 0 has the lowest resolution\n",
        "            width, height = level_dimensions[level]\n",
        "\n",
        "            # Load a region of the slide (e.g., top-left corner)\n",
        "            region = slide.read_region(location=(0, 0), level=level, size=(width // 10, height // 10))\n",
        "            image = region.convert(\"RGB\")\n",
        "\n",
        "            # Apply data augmentation\n",
        "            image = data_transforms(image)\n",
        "\n",
        "            images.append(image)\n",
        "            labels.append(label)\n",
        "    return images, labels\n",
        "\n",
        "# Load LGG and GBM images with labels\n",
        "train_lgg_images, train_lgg_labels = load_images_from_folder(\"/kaggle/input/training-lgg/lgg\", label=\"lgg\")\n",
        "train_gbm_images, train_gbm_labels = load_images_from_folder(\"/kaggle/input/gbm-training-dataset/gbm\", label=\"gbm\")\n",
        "test_images, _ = load_images_from_folder(\"/kaggle/input/test-dataset-tumor/test_folder\", label=None)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "v_-tTroizyaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load CLIP model and processor\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Move model to appropriate device\n",
        "clip_model.to(device)\n",
        "\n",
        "# Extract features for training images\n",
        "def extract_features(images, labels):\n",
        "    features = []\n",
        "    for image, label in zip(images, labels):\n",
        "        inputs = clip_processor(text=label, images=image.unsqueeze(0), return_tensors=\"pt\")\n",
        "        inputs = {key: tensor.to(device) for key, tensor in inputs.items()}  # Move inputs to device\n",
        "        with torch.no_grad():\n",
        "            image_features = clip_model.get_image_features(pixel_values=inputs[\"pixel_values\"])\n",
        "        features.append(image_features.cpu().numpy())\n",
        "    return features\n",
        "\n",
        "\n",
        "# Extract features for training and testing images\n",
        "train_lgg_features = extract_features(train_lgg_images, train_lgg_labels)\n",
        "train_gbm_features = extract_features(train_gbm_images, train_gbm_labels)\n",
        "test_features = extract_features(test_images, [None]*len(test_images))  # No labels for test set\n",
        "\n",
        "# Combine features and labels\n",
        "X_train_flat = np.concatenate((train_lgg_features, train_gbm_features)).reshape(-1, train_lgg_features[0].shape[-1])\n",
        "y_train = np.concatenate((train_lgg_labels, train_gbm_labels))\n",
        "\n",
        "# Train logistic regression classifier\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train_flat, y_train)\n",
        "\n",
        "# Make predictions\n",
        "def predict(features):\n",
        "    features_flat = features.reshape(features.shape[0], -1)\n",
        "    predictions = classifier.predict(features_flat)\n",
        "    return predictions\n",
        "\n",
        "# Evaluate performance\n",
        "def evaluate(predictions, true_labels):\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    return accuracy\n",
        "\n",
        "# Evaluate on training set\n",
        "train_predictions = predict(np.concatenate((train_lgg_features, train_gbm_features)))\n",
        "train_accuracy = evaluate(train_predictions, y_train)\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "\n",
        "# Make predictions on test set\n",
        "test_predictions = predict(np.concatenate(test_features))\n",
        "\n",
        "# Display predictions\n",
        "for i, prediction in enumerate(test_predictions):\n",
        "    if prediction == \"lgg\":\n",
        "        print(f\"Test image {i+1} is classified as LGG.\")\n",
        "    else:\n",
        "        print(f\"Test image {i+1} is classified as GBM.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "yIWCRoBtzyaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "with data augumentation and using svm and random forest for classifying"
      ],
      "metadata": {
        "id": "DYMpGLqgzyaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openslide\n",
        "import os\n",
        "import openslide\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define data augmentation transforms\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "def load_images_from_folder(folder, label):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for filename in os.listdir(folder):\n",
        "        if filename.endswith(\".svs\"):\n",
        "            path = os.path.join(folder, filename)\n",
        "            slide = openslide.OpenSlide(path)\n",
        "            level_count = slide.level_count\n",
        "            level_dimensions = slide.level_dimensions\n",
        "            level = 0  # Assuming level 0 has the lowest resolution\n",
        "            width, height = level_dimensions[level]\n",
        "\n",
        "            # Load a region of the slide (e.g., top-left corner)\n",
        "            region = slide.read_region(location=(0, 0), level=level, size=(width // 10, height // 10))\n",
        "            image = region.convert(\"RGB\")\n",
        "\n",
        "            # Apply data augmentation\n",
        "            image = data_transforms(image)\n",
        "\n",
        "            images.append(image)\n",
        "            labels.append(label)\n",
        "    return images, labels\n",
        "\n",
        "# Load LGG and GBM images with labels\n",
        "train_lgg_images, train_lgg_labels = load_images_from_folder(\"/kaggle/input/training-lgg/lgg\", label=\"lgg\")\n",
        "train_gbm_images, train_gbm_labels = load_images_from_folder(\"/kaggle/input/gbm-training-dataset/gbm\", label=\"gbm\")\n",
        "test_images, _ = load_images_from_folder(\"/kaggle/input/test-dataset-tumor/test_folder\", label=None)\n",
        "\n",
        "# Load CLIP model and processor\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Move model to appropriate device\n",
        "clip_model.to(device)\n",
        "\n",
        "# Extract features for training images\n",
        "def extract_features(images, labels):\n",
        "    features = []\n",
        "    for image, label in zip(images, labels):\n",
        "        inputs = clip_processor(text=label, images=image.unsqueeze(0), return_tensors=\"pt\")\n",
        "        inputs = {key: tensor.to(device) for key, tensor in inputs.items()}  # Move inputs to device\n",
        "        with torch.no_grad():\n",
        "            image_features = clip_model.get_image_features(pixel_values=inputs[\"pixel_values\"])\n",
        "        features.append(image_features.cpu().numpy())\n",
        "    return features\n",
        "\n",
        "\n",
        "# Extract features for training and testing images\n",
        "train_lgg_features = extract_features(train_lgg_images, train_lgg_labels)\n",
        "train_gbm_features = extract_features(train_gbm_images, train_gbm_labels)\n",
        "test_features = extract_features(test_images, [None]*len(test_images))  # No labels for test set\n",
        "\n",
        "# Combine features and labels\n",
        "X_train_flat = np.concatenate((train_lgg_features, train_gbm_features)).reshape(-1, train_lgg_features[0].shape[-1])\n",
        "y_train = np.concatenate((train_lgg_labels, train_gbm_labels))\n",
        "\n",
        "# Train SVM classifier\n",
        "svm_classifier = SVC()\n",
        "svm_classifier.fit(X_train_flat, y_train)\n",
        "\n",
        "# Train Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier()\n",
        "rf_classifier.fit(X_train_flat, y_train)\n",
        "\n",
        "# Make predictions\n",
        "def predict(classifier, features):\n",
        "    features_flat = features.reshape(features.shape[0], -1)\n",
        "    predictions = classifier.predict(features_flat)\n",
        "    return predictions\n",
        "\n",
        "# Evaluate performance\n",
        "def evaluate(predictions, true_labels):\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    return accuracy\n",
        "\n",
        "# Evaluate SVM on training set\n",
        "train_svm_predictions = predict(svm_classifier, np.concatenate((train_lgg_features, train_gbm_features)))\n",
        "train_svm_accuracy = evaluate(train_svm_predictions, y_train)\n",
        "print(\"SVM Training Accuracy:\", train_svm_accuracy)\n",
        "\n",
        "# Evaluate Random Forest on training set\n",
        "train_rf_predictions = predict(rf_classifier, np.concatenate((train_lgg_features, train_gbm_features)))\n",
        "train_rf_accuracy = evaluate(train_rf_predictions, y_train)\n",
        "print(\"Random Forest Training Accuracy:\", train_rf_accuracy)\n",
        "\n",
        "# Make predictions on test set using SVM\n",
        "test_svm_predictions = predict(svm_classifier, np.concatenate(test_features))\n",
        "\n",
        "# Display predictions\n",
        "for i, prediction in enumerate(test_svm_predictions):\n",
        "    if prediction == \"lgg\":\n",
        "        print(f\"Test image {i+1} is classified as LGG (SVM).\")\n",
        "    else:\n",
        "        print(f\"Test image {i+1} is classified as GBM (SVM).\")\n",
        "\n",
        "# Make predictions on test set using Random Forest\n",
        "test_rf_predictions = predict(rf_classifier, np.concatenate(test_features))\n",
        "\n",
        "# Display predictions\n",
        "for i, prediction in enumerate(test_rf_predictions):\n",
        "    if prediction == \"lgg\":\n",
        "        print(f\"Test image {i+1} is classified as LGG (Random Forest).\")\n",
        "    else:\n",
        "        print(f\"Test image {i+1} is classified as GBM (Random Forest).\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "tuX0QI3RzyaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "hlmn3aB7zyaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "ICg6LuxUzyaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "GD4BjKuJzyaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1TyODBt6zyaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Y9W6BgdzyaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kc2gLSiKzyaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "Fuzf1WkOzyaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "2FCWoJQvzyaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "Ix2f5XC8zyaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "uRAWgfd8zyaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "CFSA3IQHzyaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "XBAZP9MMzyaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "eq838__PzyaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "0CTZLeDLzyaR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}